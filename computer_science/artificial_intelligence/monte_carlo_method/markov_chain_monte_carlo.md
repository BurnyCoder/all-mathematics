# Markov Chain Monte Carlo

- **Mathematical Definition**: Markov Chain Monte Carlo (MCMC) methods are a class of algorithms for sampling from a probability distribution $\pi(x)$ by constructing a [Markov Chain](https://en.wikipedia.org/wiki/Markov_chain) that has $\pi(x)$ as its equilibrium distribution. A key requirement for the Markov chain is to be ergodic. The state of the chain after a large number of steps is then used as a sample from the desired distribution. The quality of the sample improves as a function of the number of steps.

  A widely used MCMC algorithm is the **Metropolis-Hastings algorithm**. Given a target probability distribution $\pi(x)$ and a proposal distribution $Q(x'|x)$, the algorithm proceeds as follows:
  1. Initialize at some state $x_0$.
  2. For $t=0, 1, 2, \dots$:
     a. Propose a new state $x'$ from the proposal distribution $Q(x'|x_t)$.
     b. Calculate the acceptance probability:
        $$ \alpha(x', x_t) = \min\left(1, \frac{\pi(x') Q(x_t|x')}{\pi(x_t) Q(x'|x_t)}\right) $$
     c. Generate a uniform random number $u \in [0, 1]$.
     d. If $u \le \alpha(x', x_t)$, accept the new state: $x_{t+1} = x'$.
     e. Otherwise, reject the new state: $x_{t+1} = x_t$.

- **Description**: MCMC methods provide a powerful way to sample from complex, high-dimensional probability distributions where direct, independent sampling is computationally expensive or impossible. They work by simulating a random walk on the state space, constructing a Markov chain where each state depends only on the previous one. This chain is designed to eventually "forget" its starting point (after a "burn-in" period) and converge to a stationary distribution, which is the target distribution we want to sample from. By collecting states from the chain after it has converged, we obtain a correlated sample from the target distribution.

- **Subfields it's part of**:
    - [Computational Statistics](https://en.wikipedia.org/wiki/Computational_statistics): MCMC is a cornerstone of modern computational statistics.
    - [Bayesian Statistics](https://en.wikipedia.org/wiki/Bayesian_statistics): It's the primary tool for sampling from posterior distributions in Bayesian inference.
    - [Statistics](https://en.wikipedia.org/wiki/Statistics): A major class of algorithms in statistical inference.
    - [Probability Theory](../probability_theory/): The theoretical underpinnings of MCMC are based in probability theory and the theory of Markov chains.
    - [Machine Learning](../../computer_science/machine_learning/): Used for inference and learning in probabilistic graphical models.
    - [Statistical Physics](https://en.wikipedia.org/wiki/Statistical_physics): Originated in statistical physics for simulating particle systems.
    - [Computer Science](../../computer_science/): A fundamental algorithm in scientific computing.

- **Subfields and concepts it includes**:
    - **[Markov Chain](https://en.wikipedia.org/wiki/Markov_chain)**: The algorithm is based on constructing a Markov chain with specific properties (ergodicity, detailed balance).
    - **[Monte Carlo method](https://en.wikipedia.org/wiki/Monte_Carlo_method)**: It is a specific type of Monte Carlo method that uses dependent samples generated by a Markov chain.
    - **Stationary Distribution**: The target distribution is designed to be the unique stationary distribution of the constructed Markov chain.
    - **Burn-in Period**: The initial iterations of the MCMC sampler that are discarded to allow the chain to reach its stationary distribution.
    - **Proposal Distribution**: In methods like Metropolis-Hastings, this distribution is used to generate candidate samples.
    - **Detailed Balance**: A sufficient condition for a Markov chain to have a desired stationary distribution. The Metropolis-Hastings acceptance probability is derived to satisfy this condition.
    - **Ergodicity**: A property of Markov chains that ensures they converge to a unique stationary distribution regardless of the starting state.
    - **[Probability Theory](../probability_theory/)**: Relies heavily on concepts like probability distributions, conditional probability, and expectation.
    - **[Linear Algebra](../../pure_mathematics/linear_algebra/)**: The convergence properties of Markov chains can be analyzed using the eigenvalues and eigenvectors of their transition matrices.
    - **[Analysis](../../pure_mathematics/analysis/)**: The continuous-space formulation of MCMC involves probability density functions, which are defined using integrals.

- **Applications**:
    - **Bayesian Inference**: Estimating posterior distributions of parameters in statistical models. For example, estimating the parameters of a logistic regression model.
    - **Statistical Physics**: Simulating complex physical systems, such as the Ising model of ferromagnetism or molecular dynamics.
    - **Computational Biology**: Inferring phylogenetic trees, analyzing protein structures, and aligning sequences.
    - **Machine Learning**: Sampling from probabilistic models, such as Bayesian neural networks, Restricted Boltzmann Machines, and Latent Dirichlet Allocation (LDA).
    - **Finance and Econometrics**: Used in financial modeling for tasks like option pricing and risk analysis.
    - **Image Processing**: Used for image restoration and segmentation through Bayesian models like Markov Random Fields.

- **More Concrete Variants**:
    - **Metropolis-Hastings Algorithm**: A general and widely used MCMC method. A simple variant is the Random Walk Metropolis-Hastings, where the proposal distribution is symmetric.
    - **Gibbs Sampling**: A special case of Metropolis-Hastings where proposals are drawn from the full conditional distributions of each variable. It is highly efficient when these conditionals are easy to sample from.
    - **Slice Sampling**: A method that avoids the need to tune a proposal distribution by sampling uniformly from the region under the probability density curve.
    - **Hamiltonian Monte Carlo (HMC)**: A sophisticated method that uses Hamiltonian dynamics from physics to propose distant states with high acceptance probability, significantly improving sampling efficiency for high-dimensional problems.
    - **Langevin Monte Carlo**: Uses Langevin dynamics, which incorporates gradient information of the target distribution to make more efficient proposals.
    - **Reversible-Jump MCMC (RJMCMC)**: An extension of MCMC to handle models where the number of parameters is itself a variable, allowing for Bayesian model selection.

- **Related Concepts and Algorithms**:
    - **[Monte Carlo method](./monte_carlo_method.md)**: The broader class of algorithms that use random sampling. MCMC is distinct in that it uses correlated samples from a Markov chain, whereas basic Monte Carlo methods often assume independent samples.
    - **[Monte Carlo Tree Search](./monte_carlo_tree_search.md)**: Another algorithm using Monte Carlo simulation, but applied to searching decision trees for optimal moves in games and planning problems.
    - **Variational Inference (VI)**: An alternative family of methods for approximating intractable posterior distributions. VI turns the inference problem into an optimization problem, which is often faster than MCMC but may be less accurate as it makes stronger assumptions about the form of the posterior.
    - **Sequential Monte Carlo (SMC) / Particle Filters**: A class of methods for sampling from a sequence of probability distributions, particularly important for online inference in state-space models and dynamic systems.
    - **[Importance Sampling](https://en.wikipedia.org/wiki/Importance_sampling)**: A technique for estimating properties of a distribution using samples from another. It's a core idea in some more advanced MCMC and SMC methods.

- **Wikipedia**: https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo
